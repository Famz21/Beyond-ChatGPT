{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-3.5-turbo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4SKfBCSB8ds",
        "outputId": "db790e6e-5133-4565-8f53-97b0a3fcfe6e"
      },
      "outputs": [],
      "source": [
        "!pip install openai cohere tiktoken -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJouXnUgKv",
        "outputId": "96d54b76-5844-465d-ae11-962d46019b86"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/authentication?lang=python) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `system`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "fc7012a7-e315-486f-b906-10c13dadcf87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9px9ZlhEAI0U42Hmau1Ly3amDTwoR', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='LangChain and LlamaIndex are two completely different entities with different purposes:\\n\\n- LangChain is a language learning platform that uses blockchain technology to provide users with a secure and decentralized way to access language learning resources, connect with tutors, and earn rewards for their language learning efforts. It aims to revolutionize the way people learn languages by making the process more efficient, accessible, and rewarding.\\n\\n- LlamaIndex, on the other hand, is a financial index that tracks the performance of assets within the cryptocurrency market. It provides investors and traders with insight into how specific cryptocurrencies are performing against each other and against other financial assets. LlamaIndex helps users make informed investment decisions by providing them with real-time data and analysis of the market.', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1722169141, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=146, prompt_tokens=19, total_tokens=165))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4o-mini\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"system\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "e871452b-0e60-4008-f700-7b1485a12641"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex are both frameworks designed to enhance the capabilities of large language models (LLMs) by providing tools and abstractions for building applications that utilize these models. However, they focus on different aspects and functionalities:\n",
              "\n",
              "### LangChain\n",
              "- **Purpose**: LangChain is primarily designed to facilitate the integration of LLMs into applications by providing a structure for building end-to-end applications that leverage natural language understanding and generation.\n",
              "- **Features**:\n",
              "  - **Chain Abstractions**: LangChain allows developers to create chains of prompts that can be executed in sequence, making it easier to manage complex workflows.\n",
              "  - **Agents**: It includes built-in functionality for creating agents that can take actions based on the model's outputs.\n",
              "  - **Memory**: LangChain provides ways to add memory to applications, allowing them to remember past interactions for more coherent conversations.\n",
              "  - **Integration**: It offers integrations with various data sources, APIs, and external services, enabling applications to pull in contextual data.\n",
              "  - **Multi-Model Support**: LangChain can work with different models and provide tools to switch between them as needed.\n",
              "\n",
              "### LlamaIndex (previously known as GPT Index)\n",
              "- **Purpose**: LlamaIndex focuses on providing a structured way to connect LLMs to external data sources, enabling efficient retrieval and indexing of information for query answering.\n",
              "- **Features**:\n",
              "  - **Data Indexing**: It allows users to create indices of documents or datasets that can be efficiently queried by the model.\n",
              "  - **Retrieval-Augmented Generation (RAG)**: LlamaIndex facilitates RAG, where models are able to retrieve relevant information from an indexed dataset in real-time to enhance their responses.\n",
              "  - **Support for Various Data Types**: It can handle both structured and unstructured data, making it versatile for different applications.\n",
              "  - **Efficiency**: The focus is on optimizing the retrieval process so that LLMs can provide answers based on up-to-date or large volumes of external information.\n",
              "\n",
              "### Summary\n",
              "In summary, **LangChain** focuses on building applications and workflows around LLMs, offering features like chaining and memory, while **LlamaIndex** emphasizes data retrieval and indexing to enhance the model's ability to answer questions based on external information. Depending on your project requirements—whether it's building a full application or improving data handling with LLMs—you might choose one or the other or even combine them for a comprehensive solution."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `system` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The system message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the system prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "05b92e0a-38f0-4ff7-ef5c-b3c2a3383980"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I don't have preferences, especially when I'm this hungry! But honestly, whether it's crushed or cubed ice, what I really need right now is some food! Can we talk about something tasty instead?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "d4ed7b30-36f4-4472-bd6d-c251bc5293e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think both have their own charm! Crushed ice is perfect for getting that refreshing chill in drinks, while cubed ice looks nice and lasts longer in a glass. What about you? Do you have a favorite?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "9106bf16-b795-4dbf-feeb-890033d82ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-9pxLqD2LJVfoOfmGpsezFZ5XWJyuG', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='I think both have their own charm! Crushed ice is perfect for getting that refreshing chill in drinks, while cubed ice looks nice and lasts longer in a glass. What about you? Do you have a favorite?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1722169902, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_ba606877f9', usage=CompletionUsage(completion_tokens=44, prompt_tokens=30, total_tokens=74))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-3.5-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "a2ec4d1d-c830-42dc-cb71-5479193212d3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple design of the chair complemented the room, while the falbean curtains added a vibrant touch of color."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "91f341ea-0dd8-44c9-9d5e-df6b10ba8322"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\"The stimple furniture in the workshop was perfectly complemented by the falbean that made assembling the pieces a breeze.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought Prompting\n",
        "\n",
        "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
        "\n",
        "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
        "\n",
        "Let's look at a simple reasoning based example without CoT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "b4e3d706-4e08-402b-c308-446b53f980dc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To determine if it matters which travel option Billy selects, we need to compare the total travel times for both options, factoring in the time zones.\n",
              "\n",
              "First, we confirm the current local time in San Francisco. San Francisco is in the Pacific Time Zone (PT), which is UTC-7 during Daylight Saving Time. Since it's currently 1 PM local time in San Francisco, that means it is 4 PM EDT (Eastern Daylight Time) in New York.\n",
              "\n",
              "Now, let's evaluate the two options:\n",
              "\n",
              "1. **Option 1 (Flying + Bus):**\n",
              "   - Flight time: 3 hours\n",
              "   - Bus time: 2 hours\n",
              "   - Total travel time: 3 hours + 2 hours = 5 hours\n",
              "   - Departure time from San Francisco: 1 PM PT\n",
              "   - Arrival time in New York (EDT): \n",
              "     - 1 PM PT + 5 hours = 6 PM PT ()\n",
              "     - Convert 6 PM PT to EDT: 6 PM PT + 3 hours = 9 PM EDT\n",
              "\n",
              "2. **Option 2 (Teleporter + Bus):**\n",
              "   - Teleportation time: 0 hours\n",
              "   - Bus time: 1 hour\n",
              "   - Total travel time: 0 hours + 1 hour = 1 hour\n",
              "   - Departure time from San Francisco: 1 PM PT\n",
              "   - Arrival time in New York (EDT): \n",
              "     - 1 PM PT + 1 hour = 2 PM PT\n",
              "     - Convert 2 PM PT to EDT: 2 PM PT + 3 hours = 5 PM EDT\n",
              "\n",
              "Now to see if either option gets Billy home before 7 PM EDT:\n",
              "- **Option 1 arrives at 9 PM EDT,** which is after 7 PM EDT.\n",
              "- **Option 2 arrives at 5 PM EDT,** which is before 7 PM EDT.\n",
              "\n",
              "Thus, yes, it does matter which travel option Billy selects: **Choosing the teleporter followed by the bus is the only option that gets Billy home before 7 PM EDT.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "HpKEt7Z5fo4s",
        "outputId": "c25a982f-afd5-4154-b2a2-388029f3e4e9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To determine if it matters which travel option Billy selects, we first need to establish the current time based on where Billy is, calculate the total travel times for each option, and then see if he can make it home before 7 PM EDT.\n",
              "\n",
              "### Step 1: Identify current local time and travel timezone\n",
              "- It is currently 1 PM local time in San Francisco (PDT, which is UTC-7). \n",
              "- To convert this to EDT (Eastern Daylight Time, which is UTC-4):\n",
              "  - 1 PM PDT + 3 hours = 4 PM EDT.\n",
              "\n",
              "### Step 2: Determine the deadlines\n",
              "- Billy needs to reach home before 7 PM EDT.\n",
              "\n",
              "### Step 3: Calculate total travel time for each option\n",
              "**Option 1: Fly then take a bus**\n",
              "- Flight time: 3 hours\n",
              "- Bus time: 2 hours\n",
              "- Total travel time = 3 hours (flight) + 2 hours (bus) = 5 hours\n",
              "\n",
              "Since it's currently 1 PM PDT:\n",
              "- Departure time from San Francisco = 1 PM PDT\n",
              "- 5 hours of travel means Billy would arrive at:\n",
              "  - 1 PM + 5 hours = 6 PM PDT \n",
              "- To convert 6 PM PDT to EDT:\n",
              "  - 6 PM PDT + 3 hours = 9 PM EDT\n",
              "\n",
              "**Option 2: Teleport then take a bus**\n",
              "- Teleport time: 0 hours\n",
              "- Bus time: 1 hour\n",
              "- Total travel time = 0 hours (teleport) + 1 hour (bus) = 1 hour\n",
              "\n",
              "Since it's still 1 PM PDT:\n",
              "- Departure time from San Francisco = 1 PM PDT\n",
              "- 1 hour of travel means Billy would arrive at:\n",
              "  - 1 PM + 1 hour = 2 PM PDT \n",
              "- To convert 2 PM PDT to EDT:\n",
              "  - 2 PM PDT + 3 hours = 5 PM EDT\n",
              "\n",
              "### Step 4: Compare arrival times with the deadline\n",
              "- For Option 1 (Fly + bus): Arrival time = 9 PM EDT\n",
              "- For Option 2 (Teleport + bus): Arrival time = 5 PM EDT\n",
              "\n",
              "### Conclusion\n",
              "Yes, it does matter which travel option Billy selects. \n",
              "\n",
              "- If he takes the **fly then bus option**, he arrives at **9 PM EDT**, which is **after** the deadline.\n",
              "- If he takes the **teleport then bus option**, he arrives at **5 PM EDT**, which is **before** the deadline.\n",
              "\n",
              "Therefore, Billy should choose the teleport option to reach home before 7 PM EDT."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHH8zof-gkc1"
      },
      "source": [
        "With the addition of a single phrase `\"Think through your response step by step.\"` we're able to completely turn the response around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-3.5-turbo` through an API, developer style, let's move on to creating a simple application powered by `gpt-3.5-turbo`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/Beyond-ChatGPT/tree/main) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
